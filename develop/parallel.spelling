source/parallel.rst:7: (infiniband)  zCFD uses Intel MPI (Message Passing Interface) to enable running over multiple machines or multiple GPUs, either in one machine or across multiple. A paid license is required for this functionality. This is achieved by decomposing the domain into parallel regions that are distributed to each MPI rank. MPI can leverage high bandwidth network interconnects such as infiniband or AWS’ Elastic Fabric adaptor,
source/parallel.rst:15: (zcfd)  To run in parallel you add the -n option to the run_zcfd script. The provided argument is the number of MPI processes to launch.
source/parallel.rst:18: (MPI)  Hybrid MPI/OpenMP
source/parallel.rst:18: (OpenMP)  Hybrid MPI/OpenMP
source/parallel.rst:19: (openmp)  This is the default mode for zCFD. It will auto detect the number of sockets on each node and set the number of openmp threads appropriately. The –tpn option to run_zcfd can be used to set the number of MPI processes to launch on each node. The number of openmp threads will be set automatically by dividing the total number of cores between all processes.
source/parallel.rst:19: (tpn)  This is the default mode for zCFD. It will auto detect the number of sockets on each node and set the number of openmp threads appropriately. The –tpn option to run_zcfd can be used to set the number of MPI processes to launch on each node. The number of openmp threads will be set automatically by dividing the total number of cores between all processes.
source/parallel.rst:19: (zcfd)  This is the default mode for zCFD. It will auto detect the number of sockets on each node and set the number of openmp threads appropriately. The –tpn option to run_zcfd can be used to set the number of MPI processes to launch on each node. The number of openmp threads will be set automatically by dividing the total number of cores between all processes.
source/parallel.rst:19: (openmp)  This is the default mode for zCFD. It will auto detect the number of sockets on each node and set the number of openmp threads appropriately. The –tpn option to run_zcfd can be used to set the number of MPI processes to launch on each node. The number of openmp threads will be set automatically by dividing the total number of cores between all processes.
source/parallel.rst:21: (MPI)  For example, if you had two hosts with 2 x 12 core CPUs per host and you wanted to run Hybrid MPI/OpenMP you would request 1 MPI rank per CPU socket and zCFD will automatically set the number of OpenMP threads per rank to match the number of cores per socket (in our case 12).
source/parallel.rst:21: (OpenMP)  For example, if you had two hosts with 2 x 12 core CPUs per host and you wanted to run Hybrid MPI/OpenMP you would request 1 MPI rank per CPU socket and zCFD will automatically set the number of OpenMP threads per rank to match the number of cores per socket (in our case 12).
source/parallel.rst:28: (openmp)  The number of openmp threads assigned to each rank can be overridden by setting 
source/parallel.rst:45: (Slurm)  There are many cluster scheduling systems but commonly used ones are Slurm, PBS Pro or GridEngine. Please refer to either the cluster scheduling software or your local HPC systems’ documentation for how to submit jobs to the appropriate nodes on your system as configuration varies between machines.
source/parallel.rst:47: (autodetect)  Intel MPI has tight integration to most commonly used scheduling systems and so will typically autodetect the nodes to run on.
source/parallel.rst:53: (hostnames)   can be set in the environment to point at a file containing the list of hostnames that zCFD will launch on. For example:
source/parallel.rst:83: (Libfabric)  Libfabric provider selection
source/parallel.rst:84: (autodetect)  zCFD will attempt to autodetect the correct provider to use for MPI communication. It tries the following providers in order:
source/parallel.rst:95: (autodetection)  If FI_PROVIDER is set then the autodetection is skipped.
source/parallel.rst:98: (libfabric)  Using cluster provided libfabric
source/parallel.rst:100: (libfabric)  By default zCFD will use the version of libfabric that ships with Intel MPI. For some advanced use cases you may wish to make use of preinstalled libfabric. This may be if you have a custom network driver or a specific configuration on your cluster.
source/parallel.rst:100: (libfabric)  By default zCFD will use the version of libfabric that ships with Intel MPI. For some advanced use cases you may wish to make use of preinstalled libfabric. This may be if you have a custom network driver or a specific configuration on your cluster.
source/parallel.rst:108: (Pre)  Pre launch script
source/parallel.rst:109: (zcfd)  On some systems the defaults setup by zcfd may not be optimal and so a custom script to modify the environment can be injected by setting the environment variable 
source/parallel.rst:115: (openmp)  By default zCFD binds its openmp threads to cores using the 
source/parallel.rst:120: (doesn)  On platforms where Intel MPI doesn’t work then we can build zCFD against a specific implementation of MPI. This would be a custom deployment for your facility so get in touch with us if you need this.
source/parallel.rst:124: (filesystem)  As zCFD is partly written in python, the install consists of a large quantity of small files, which on a large scale cluster can have a negative impact on the cluster filesystem during startup. In order to mitigate this it is possible to stage the code into either local storage or memory on the compute nodes using MPI to broadcast the data.
