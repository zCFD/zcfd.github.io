source/execution.rst:11: (Slurm)  zCFD includes an in-built command-line environment, which should be activated either when run directly or via a cluster management and job scheduling system, such as Slurm.
source/execution.rst:57: (py)  The name of the control dictionary (<control_dict>.py)
source/execution.rst:110: (py)  An example override.py file for a background domain with a single rotating turbine would be:
source/execution.rst:120: (Slurm)  To run zCFD on a compute cluster with a queuing system (such as Slurm), the command-line environment activation is included within the submission script:
source/execution.rst:122: (Slurm)  Example Slurm submission script “run_zcfd.sub”:
source/execution.rst:122: (zcfd)  Example Slurm submission script “run_zcfd.sub”:
source/execution.rst:140: (num)  In this system we have 2 nodes, each with 2 GPUs.  We allocate one task per GPU, giving a total of 4 tasks.  Note that num_tasks, in this case 4, on the last line should match Slurm “ntasks” on line 5.  The case is, optionally, run in “exclusive” mode which means that zCFD has uncontended use of the devices.  In this case we have a 64-core CPU, giving 16 CPU cores for each of the 4 tasks.  The “gres” line tells zCFD that there are 2 GPUs available on each node.
source/execution.rst:140: (Slurm)  In this system we have 2 nodes, each with 2 GPUs.  We allocate one task per GPU, giving a total of 4 tasks.  Note that num_tasks, in this case 4, on the last line should match Slurm “ntasks” on line 5.  The case is, optionally, run in “exclusive” mode which means that zCFD has uncontended use of the devices.  In this case we have a 64-core CPU, giving 16 CPU cores for each of the 4 tasks.  The “gres” line tells zCFD that there are 2 GPUs available on each node.
source/execution.rst:140: (ntasks)  In this system we have 2 nodes, each with 2 GPUs.  We allocate one task per GPU, giving a total of 4 tasks.  Note that num_tasks, in this case 4, on the last line should match Slurm “ntasks” on line 5.  The case is, optionally, run in “exclusive” mode which means that zCFD has uncontended use of the devices.  In this case we have a 64-core CPU, giving 16 CPU cores for each of the 4 tasks.  The “gres” line tells zCFD that there are 2 GPUs available on each node.
source/execution.rst:140: (uncontended)  In this system we have 2 nodes, each with 2 GPUs.  We allocate one task per GPU, giving a total of 4 tasks.  Note that num_tasks, in this case 4, on the last line should match Slurm “ntasks” on line 5.  The case is, optionally, run in “exclusive” mode which means that zCFD has uncontended use of the devices.  In this case we have a 64-core CPU, giving 16 CPU cores for each of the 4 tasks.  The “gres” line tells zCFD that there are 2 GPUs available on each node.
source/execution.rst:140: (gres)  In this system we have 2 nodes, each with 2 GPUs.  We allocate one task per GPU, giving a total of 4 tasks.  Note that num_tasks, in this case 4, on the last line should match Slurm “ntasks” on line 5.  The case is, optionally, run in “exclusive” mode which means that zCFD has uncontended use of the devices.  In this case we have a 64-core CPU, giving 16 CPU cores for each of the 4 tasks.  The “gres” line tells zCFD that there are 2 GPUs available on each node.
source/execution.rst:148: (Slurm)  The job can then be managed using the standard Slurm commands.
source/execution.rst:164: (Juypter)  Using Juypter
